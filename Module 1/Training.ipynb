{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:32:19.529461Z",
     "start_time": "2022-03-25T13:32:18.636463Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "from util.LoadDataOCD import LoadsegDBcrop_nia_paper\n",
    "from util.losses import ComboLoss\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from PIL import Image\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:36:09.638284Z",
     "start_time": "2022-03-25T13:36:09.626285Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"------------------------ GPU Set ---------------------------\"\"\"\n",
    "gpu_device = '0' # GPU 번호가 1번인 GPU 사용\n",
    "\"\"\"------------------------ model parameters  ---------------------------\"\"\"\n",
    "batch_size = 2\n",
    "num_epoch = 50 # epoch\n",
    "model_mode = 'DeepRes101' # DeepRes101 / DeepRes50 / DeepFCN101\n",
    "\n",
    "\"\"\"------------------------ function parameters ---------------------------\"\"\"\n",
    "weight_decay = 0.0005 # weight 감소량\n",
    "learning_rate= 1e-3\n",
    "momentum = 0.9\n",
    "num_classes = 7  # 0:NO building, 1:소형, 2:아파트, 3:공장, 4:중형단독시설, 5:대형시설, 6:contour\n",
    "power = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:36:11.731283Z",
     "start_time": "2022-03-25T13:36:11.715284Z"
    }
   },
   "outputs": [],
   "source": [
    "img_format = '.png'\n",
    "gt_format= '.png'\n",
    "input_size = 512 # 학습으로 들어가는 영상 크기\n",
    "original_size = 1024 # 데이터셋 영상 크기\n",
    "csv_data = '/Projects/AISpark_Challenge_IRDIS/Data/AIHub/Training/train_buildings.csv'\n",
    "data_directory = '/Projects/AISpark_Challenge_IRDIS/Data/AIHub/Training'\n",
    "img_folder_name = 'train_buildings_data' # DATA_DIRECTORY 내 image 가 들어있는 폴더 이름\n",
    "label_folder_name = 'train_buildings_labeling' # DATA_DIRECTORY 내 GT image 가 들어있는 폴더 이름\n",
    "\n",
    "save_pred_every= 20 # 해당 iter 마다 한번씩 저장\n",
    "snapshot_dir = '/Projects/AISpark_Challenge_IRDIS/Data/AIHub/Training/snapshots_nia_building_resnet50_contour_test/' # 저장 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:36:11.986285Z",
     "start_time": "2022-03-25T13:36:11.975286Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']= gpu_device\n",
    "automated_log_path = snapshot_dir + \"log_building.txt\" # log 저장 이름\n",
    "INPUT_SIZE_m = [input_size, input_size]\n",
    "original_size = [original_size, original_size]  # width, height\n",
    "IMG_MEAN = np.array((128, 128, 128), dtype=np.float32) # 학습 속도를 위해 RGB 128을 영점으로 둔다. [-128~127], Load code에서 128로 나눔 [-1~0.999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:36:50.537019Z",
     "start_time": "2022-03-25T13:36:50.529024Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"------------------------ Learning Rate Option ---------------------------\"\"\"\n",
    "def lr_poly(base_lr, iter, max_iter, power):\n",
    "    return base_lr * ((1 - float(iter) / max_iter) ** (power))\n",
    "\n",
    "def adjust_learning_rate(optimizer, i_iter):\n",
    "    lr = lr_poly(learning_rate, i_iter, num_epoch, power)\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    if len(optimizer.param_groups) > 1:\n",
    "        optimizer.param_groups[1]['lr'] = lr * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"------------------------ Extraction of matching Label  ---------------------------\"\"\"\n",
    "\n",
    "def LabelTranformer(dataName, label_folder_name):\n",
    "    \"\"\"\n",
    "    dataName: image, label, name 의 정보들\n",
    "    data_name: post, pre 를 제외한 데이터 이름\n",
    "    \"\"\"\n",
    "    label_set = []\n",
    "    for i in range(dataName['name'].__len__()):\n",
    "\n",
    "        # Label (GT) image open\n",
    "        label_building = Image.open(label_folder_name + '/' + dataName['name'][i] + gt_format)\n",
    "\n",
    "        if dataName[\"switching\"][i] >= 0.5:\n",
    "            label_building = label_building.transpose(Image.ROTATE_180)\n",
    "\n",
    "        label = np.zeros((INPUT_SIZE_m[0], INPUT_SIZE_m[1], num_classes), np.float32)\n",
    "        # label (buildings)\n",
    "        label_building = label_building.crop(\n",
    "            (int(dataName[\"left\"][i]), int(dataName[\"top\"][i]), int(dataName[\"W\"][i]), int(dataName[\"H\"][i])))\n",
    "        label_building = np.asarray(label_building, np.float32)\n",
    "        for j in range(num_classes):\n",
    "            idx_i, idx_j = np.where(label_building[:, :] == j)[:2]\n",
    "            label[idx_i, idx_j, j] = 1.0  # building ID\n",
    "\n",
    "        label_set.append(label.copy())\n",
    "\n",
    "    return label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"------------------------ MAIN  ---------------------------\"\"\"\n",
    "def main():\n",
    "    cudnn.enabled = True\n",
    "\n",
    "    \"\"\"-------------------------- 개발 MODEL ROAD --------------------------\"\"\"\n",
    "    # DeepRes101 / DeepRes50 / DeepFCN101\n",
    "    if model_mode == 'DeepRes101':\n",
    "        model = models.segmentation.deeplabv3_resnet101(pretrained=False, num_classes=num_classes)\n",
    "    elif model_mode == 'DeepRes50':\n",
    "        model = models.segmentation.deeplabv3_resnet50(pretrained=False, num_classes=num_classes)\n",
    "    elif model_mode == 'DeepFCN101':\n",
    "        model = models.segmentation.fcn_resnet101(pretrained=False, num_classes=num_classes)\n",
    "    else:\n",
    "        raise Exception(\"Please select a model\")\n",
    "\n",
    "    model.cuda(0)\n",
    "    #     model=nn.DataParallel(model)\n",
    "    model.train()\n",
    "    # 쉽게 true로 두면 비용(memory 등) 이 더 들지만 성능이 향상됨.\n",
    "    cudnn.benchmark = True  # cudnn.benchmark = true -- uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms.\n",
    "    # -- If this is set to false, uses some in-built heuristics that might not always be fastest.\n",
    "\n",
    "    \"\"\"-------------------------- FILE SAVE --------------------------\"\"\"\n",
    "    if not os.path.exists(snapshot_dir):\n",
    "        os.makedirs(snapshot_dir)\n",
    "    # log 값을 기록\n",
    "    with open(automated_log_path, \"a\") as myfile:\n",
    "        myfile.write(\"Epoch\\t\\titer\\t\\tloss\")\n",
    "\n",
    "    \"\"\"-------------------------- FILE LOAD --------------------------\"\"\"\n",
    "\n",
    "    name_list = []\n",
    "    #     f = open(args.data_dir + args.csv_data, 'r')\n",
    "    f = open(csv_data, 'r')\n",
    "    names = csv.reader(f)\n",
    "    for name in names:\n",
    "        name[0] = name[0] + gt_format  # csv 파일 list 에 확장자가 빠진 이름들의 list 이므로\n",
    "        name_list.append(name[0])\n",
    "    f.close()\n",
    "\n",
    "    image_list_imgs = name_list\n",
    "\n",
    "    ITER_SIZE = int(image_list_imgs.__len__() / batch_size)  # training dataset 갯수 / batch_size\n",
    "\n",
    "    trainloader = data.DataLoader(\n",
    "        LoadsegDBcrop_nia_paper(data_directory,img_folder_name, image_list_imgs, label_folder_name,\n",
    "                                mean=IMG_MEAN, crop_size=INPUT_SIZE_m, img_size=original_size,\n",
    "                                scale=False),\n",
    "        batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "\n",
    "    # implement model.optim_parameters(args) to handle different models' lr setting\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    loss_function = ComboLoss({'dice': 1, 'focal': 1}, per_image=True).cuda(0)\n",
    "\n",
    "    \"\"\"----------------------- TRAINING START ------------------------\"\"\"\n",
    "    for i_iter in range(num_epoch):\n",
    "        trainloader_iter = iter(trainloader)\n",
    "        # 파라미터를 학습 하겠다.\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        adjust_learning_rate(optimizer, i_iter)\n",
    "\n",
    "        for sub_i in range(ITER_SIZE):\n",
    "            \"\"\"----------------------- LOAD DATA ------------------------\"\"\"\n",
    "            batch = trainloader_iter.next()\n",
    "            # image 하나 뽑기 pre or post image\n",
    "            image_input, datafiles, data_idx = batch  # img.copy(), label_json_info, np.array(size), data_name\n",
    "\n",
    "            image_input = image_input.transpose(1, 3)\n",
    "            image_input = image_input.transpose(2, 3)\n",
    "            image_input = Variable(image_input).cuda(0)\n",
    "            # pair image 와 label load\n",
    "            labels = LabelTranformer(datafiles, data_directory + '/' + label_folder_name)\n",
    "            # label\n",
    "            labels = torch.tensor(labels)\n",
    "            labels = labels.transpose(1, 3)\n",
    "            labels = labels.transpose(2, 3)\n",
    "            labels = Variable(labels).cuda(0)\n",
    "\n",
    "            \"\"\"----------------------- RESULTS ------------------------\"\"\"\n",
    "            pred_comb = model(image_input)\n",
    "\n",
    "            # pred_comb = torch.squeeze(pred_comb) # 불필요한 1차원 제거\n",
    "            # labels = labels.unsqueeze(1) # 필요한 1번째 차원 증가\n",
    "\n",
    "            \"\"\"----------------------- BACKWARD ------------------------\"\"\"\n",
    "            loss_target = loss_function(pred_comb['out'], labels)\n",
    "            # loss_target = loss_function(pred_comb, labels)\n",
    "\n",
    "            # proper normalization\n",
    "            loss = loss_target\n",
    "            \"\"\" source  loss, backward for differention \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  # weight update\n",
    "\n",
    "            print('sub_i = {0:3d}/{1:3d},   epoch = {2:3d}/{3:3d},   loss = {4:.3f}'.format(sub_i, ITER_SIZE, i_iter,\n",
    "                                                                                            num_epoch, loss))\n",
    "            \"\"\"----------------------- SAVE WEIGHT FILE ------------------------\"\"\"\n",
    "\n",
    "            with open(automated_log_path, \"a\") as myfile:  # 원래 있던 값에 덮어쓰기\n",
    "                myfile.write(\"\\n%d\\t\\t%d\\t\\t%.3f\" % (i_iter, sub_i, loss))\n",
    "\n",
    "            if sub_i % save_pred_every == 0 and sub_i != 0:\n",
    "                print('taking snapshot ...')\n",
    "                torch.save(model.state_dict(),\n",
    "                           os.path.join(snapshot_dir, 'sn6_resunet50_' + str(sub_i) + '_ep_' + str(i_iter) + '.pth'))\n",
    "\n",
    "        print('exp = {}'.format(snapshot_dir))\n",
    "\n",
    "\n",
    "        if i_iter != 0:\n",
    "            print('taking snapshot ...')\n",
    "            torch.save(model.state_dict(), os.path.join(snapshot_dir, 'sn6_resunet50_' + str(i_iter) + '.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}